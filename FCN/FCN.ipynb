{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4d43O13ybCdB"},"source":["# Clone FCN Function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrTzocmltNzr","executionInfo":{"status":"ok","timestamp":1609218085843,"user_tz":-480,"elapsed":1004,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"6cb3b254-007b-4599-a621-b399f3b3d70f"},"source":["!git clone https://github.com/ARG-NCTU/FCN-pytorch.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'FCN-pytorch' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gMudoC93tUOJ"},"source":["import sys\n","sys.path.append('/content/FCN-pytorch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5raeS1frbHHw"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"id":"W6dV0xCltyzh"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import scipy.misc\n","import random\n","import sys\n","if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n","    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n","import cv2\n","from torch.optim import lr_scheduler                                                                                                                                                                                       \n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","import os\n","\n","import logging\n","from zipfile import ZipFile\n","import gdown"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKFf4VgsyfsI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609218085844,"user_tz":-480,"elapsed":943,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"a6b395b0-02df-4224-8fca-eaf685ed22b2"},"source":["use_cuda = torch.cuda.is_available()\r\n","print(use_cuda)\r\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R719aNUxbLmA"},"source":["# Define FCN16s model for deconvolution layers\n","\n","\n","torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n","\n","\n","\n","*   in_channels – the number of channels of the input signal\n","*   out_channels – the number of channels generated by convolution\n","*   kerner_size – size of convolution kernel\n","*   stride – Convolution step size, that is, the multiple of the input to be expanded.\n","*   padding – The height and width are increased by 2 * padding\n","*   output_padding – the height and width are increased by padding\n","*   groups – number of blocked connections from input channel to output channel\n","*   bias – if bias = True, add bias\n","*   dilation – spacing between elements of the convolution kernel\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NV-JrUkUt8iG"},"source":["class FCN16s(nn.Module):\n","\n","    def __init__(self, pretrained_net, n_class):\n","        super(FCN16s, self).__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace = True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        \n","        # After the feature extraction layer of vgg, you can get the feature map. \n","        # The size of the feature map after 5 max_pools are respectively\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n","        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n","        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n","        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xb-80rs5bwQS"},"source":["# Define VGG16 for convolution layers"]},{"cell_type":"code","metadata":{"id":"YX9JQuVzt-Tf"},"source":["class VGGNet(VGG):\n","    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n","        super(VGGNet, self).__init__(make_layers(cfg[model]))\n","        self.ranges = ranges[model]\n","\n","        if pretrained:\n","            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n","\n","        if not requires_grad:\n","            for param in super().parameters():\n","                param.requires_grad = False\n","\n","        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n","            del self.classifier\n","\n","        if show_params:\n","            for name, param in self.named_parameters():\n","                print(name, param.size())\n","\n","    def forward(self, x):\n","        output = {}\n","\n","        # get the output of each maxpooling layer (5 maxpool in VGG net)\n","        for idx in range(len(self.ranges)):\n","            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):      \n","                x = self.features[layer](x)\n","            output[\"x%d\"%(idx+1)] = x\n","        return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDGBWJFZuA4v"},"source":["ranges = {\n","    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n","    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n","    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n","    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n","}\n","\n","# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n","cfg = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"884NY8wvuGW5"},"source":["batch_size = 8\n","epochs     = 200  #500\n","lr         = 1e-4\n","momentum   = 0\n","w_decay    = 1e-5\n","step_size  = 50\n","gamma      = 0.5\n","model_use  = \"subt_model\" # \"subt_model\"\n","n_class = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HHX215XbzSZ"},"source":["# Download_dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ChD8YP-uIpf","executionInfo":{"status":"ok","timestamp":1609218086228,"user_tz":-480,"elapsed":1314,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"1e632f91-dc34-495c-87a5-116b18103053"},"source":["dataset_url = \"https://drive.google.com/u/1/uc?id=1xdtiKMLVbNancDhIAgYDRT8okfuYy5dB&export=download\"\n","dataset_name = \"data\"\n","if not os.path.isdir(dataset_name):\n","    gdown.download(dataset_url, output=dataset_name + '.zip', quiet=False)\n","    zip1 = ZipFile(dataset_name + '.zip')\n","    zip1.extractall(dataset_name)\n","    zip1.close()\n","    os.remove(\"data.zip\")\n","\n","print(\"Finished downloading dataset.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished downloading dataset.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qX64ri-Eb1DS"},"source":["# Define path, directory trainning environment"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIwsXeyPuNcB","executionInfo":{"status":"ok","timestamp":1609218090027,"user_tz":-480,"elapsed":5105,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"b4d056cd-e5d4-4a9d-e0f1-0cd6bb3a8cc1"},"source":["# get data\n","FullPath = os.getcwd()\n","data_dir  = os.path.join(FullPath + \"/data/FCN_train\")\n","if not os.path.exists(data_dir):\n","    print(\"Data not found!\")\n","    \n","# create dir for model\n","model_dir = os.path.join(FullPath + \"/models\", model_use)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","    \n","# create dir for score\n","score_dir = os.path.join(FullPath + \"/scores\", model_use)\n","if not os.path.exists(score_dir):\n","    os.makedirs(score_dir)\n","\n","use_gpu = torch.cuda.is_available()\n","num_gpu = list(range(torch.cuda.device_count()))\n","\n","vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n","fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n","\n","if use_gpu:\n","    ts = time.time()\n","    vgg_model = vgg_model.cuda()\n","    fcn_model = fcn_model.cuda()\n","    fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n","    print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n","else:\n","#     nn.DataParallel(fcn_model)\n","    print(\"Use CPU to train.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finish cuda loading, time elapsed 0.025197267532348633\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PRaqKZLTb3YY"},"source":["# Visualize model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5DQWDbQIvSLI","executionInfo":{"status":"ok","timestamp":1609218090028,"user_tz":-480,"elapsed":5098,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"98df24bd-29f8-4444-86de-28ce4c673653"},"source":["print(fcn_model)\n","params = list(fcn_model.parameters())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DataParallel(\n","  (module): FCN16s(\n","    (pretrained_net): VGGNet(\n","      (features): Sequential(\n","        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (6): ReLU(inplace=True)\n","        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (8): ReLU(inplace=True)\n","        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (11): ReLU(inplace=True)\n","        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (13): ReLU(inplace=True)\n","        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (15): ReLU(inplace=True)\n","        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (18): ReLU(inplace=True)\n","        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (20): ReLU(inplace=True)\n","        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (22): ReLU(inplace=True)\n","        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (25): ReLU(inplace=True)\n","        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (27): ReLU(inplace=True)\n","        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (29): ReLU(inplace=True)\n","        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","    )\n","    (relu): ReLU(inplace=True)\n","    (deconv1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (classifier): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KDwetfRAb6NB"},"source":["# Dataset class\n","torch.utils.data.Dataset is an abstract class representing a dataset.\n","Your custom dataset should inherit Dataset and override the following methods:\n","\n","\n","*   __len__ so that len(dataset) returns the size of the dataset.\n","\n","*   __getitem__ to support the indexing such that dataset[i] can be used to get $i$\\ th sample\n","\n","Let's create a dataset class for our face landmarks dataset.\n","\n","We will read the csv in __init__ but leave the reading of images to __getitem__.\n","\n","This is memory efficient because all the images are not stored in the memory at once but read as required.\n"]},{"cell_type":"code","metadata":{"id":"QWJvf-WyvTX7"},"source":["means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n","h, w      = 480, 640\n","val_h     = h\n","val_w     = w\n","\n","class product_dataset(Dataset):\n","\n","    def __init__(self, root, phase, n_class=n_class, flip_rate=0.):\n","        data_dir = os.path.join(root, phase)\n","        self.rgb_list = os.listdir(os.path.join(data_dir, 'images'))\n","        _list = self.rgb_list\n","        self.label_list = []\n","        for i in range(len(self.rgb_list)):\n","            self.label_list.append(_list[i].split(\".\")[0] + \".png\")\n","\n","        self.rgb_dir = os.path.join(data_dir, 'images')\n","        self.label_dir = os.path.join(data_dir, 'masks')\n","        self.means     = means\n","        self.n_class   = n_class\n","        self.flip_rate = flip_rate\n","        if phase == 'train':\n","            self.flip_rate = 0.5\n","            \n","    def __len__(self):\n","        return len(self.rgb_list)\n","\n","    def __getitem__(self, idx):\n","        idx = idx % len(self.rgb_list)        \n","        img = cv2.imread(os.path.join(self.rgb_dir, self.rgb_list[idx]),cv2.IMREAD_UNCHANGED)\n","        label = cv2.imread(os.path.join(self.label_dir, self.label_list[idx]), cv2.IMREAD_GRAYSCALE)\n","        # print(label.shape)\n","        \n","        #===SubT label error===backpack = 38, survivor = 75, vent = 113, phone = 14\n","        label[label == 38] = 1\n","        label[label == 75] = 2\n","        label[label == 113] = 3\n","        label[label == 14] = 4  \n","        #===SubT error===       \n","        \n","        img = cv2.resize(img, (640, 480), interpolation=cv2.INTER_CUBIC)\n","        label = cv2.resize(label, (640, 480), interpolation=cv2.INTER_CUBIC)\n","   \n","        origin_img = img\n","        if random.random() < self.flip_rate:\n","            img   = np.fliplr(img)\n","            label = np.fliplr(label)\n","\n","        # reduce mean\n","        img = img[:, :, ::-1]  # switch to BGR\n","        \n","        img = np.transpose(img, (2, 0, 1)) / 255.\n","        img[0] -= self.means[0]\n","        img[1] -= self.means[1]\n","        img[2] -= self.means[2]\n","\n","        # convert to tensor\n","        img = torch.from_numpy(img.copy()).float()\n","        label = torch.from_numpy(label.copy()).long()\n","\n","        # create one-hot encoding\n","        h, w = label.size()\n","        target = torch.zeros(self.n_class, h, w)\n","        \n","        for i in range(n_class):\n","            target[i][label == i] = 1\n","        \n","#         target[0][label == 0] = 1\n","#         print(np.unique(label))\n","        \n"," \n","        sample = {'X': img, 'Y': target, 'l': label, 'origin': origin_img}\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETVvZqDScGzm"},"source":["# Define dataloader and optimizer"]},{"cell_type":"code","metadata":{"id":"WANP-zQfvWM8"},"source":["# initial dataloader for trainning and validation\n","train_data = product_dataset(data_dir, phase = 'train')\n","val_data   = product_dataset(data_dir, phase = 'test', flip_rate = 0)\n","dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=True, num_workers = 0)\n","val_loader = DataLoader(val_data, batch_size = 1, num_workers = 0)\n","\n","dataiter = iter(val_loader)\n","\n","# define loss function\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.RMSprop(fcn_model.parameters(), lr = lr, momentum = momentum, weight_decay = w_decay)\n","# decay LR by a factor of 0.5 every step_size = 50 epochs\n","scheduler = lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6puxNHwcJLG"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"rzMVhccwvX4a"},"source":["def train():\n","    for epoch in range(epochs):\n","        # print('1')\n","        fcn_model.train()\n","        # print('2')\n","        scheduler.step()\n","        configs    = \"FCNs_{}_batch{}_epoch{}_RMSprop_lr{}\"\\\n","            .format(model_use, batch_size, epoch, lr)\n","        model_path = os.path.join(model_dir, configs)\n","        \n","        ts = time.time()\n","        for iter, batch in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            if use_gpu:\n","                inputs = Variable(batch['X'].cuda())\n","                labels = Variable(batch['Y'].cuda())\n","            else:\n","                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n","            # print('3')\n","\n","            outputs = fcn_model(inputs)\n","            # print('4')\n","            loss = criterion(outputs, labels)\n","            # print('5')\n","            loss.backward()\n","            # print('6')\n","            optimizer.step()\n","            # print('7')\n","\n","            if iter % 1 == 0:\n","                print(\"epoch{}, iter{}, loss: {}\".format(epoch+1, iter, loss.item()))\n","        \n","        print(\"Finish epoch {}, time elapsed {}\".format(epoch+1, time.time() - ts))\n","        if epoch % 1 == 0:\n","            torch.save(fcn_model.state_dict(),model_path + '.pkl')\n","\n","        val(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eaU8ANsqEgY"},"source":["def val(epoch):\n","    fcn_model.eval()\n","    TP = np.zeros(n_class-1)\n","    FN = np.zeros(n_class-1)\n","    FP = np.zeros(n_class-1)\n","    total_ious = []\n","    pixel_accs = []\n","    for iter, batch in enumerate(val_loader):\n","        if use_gpu:\n","            inputs = Variable(batch['X'].cuda())\n","        else:\n","            inputs = Variable(batch['X'])\n","\n","        output = fcn_model(inputs)\n","        output = output.data.cpu().numpy()\n","\n","        N, _, h, w = output.shape\n","        pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n","\n","        target = batch['l'].cpu().numpy().reshape(N, h, w)\n","        for p, t in zip(pred, target):\n","            pixel_accs.append(pixel_acc(p, t))\n","            _TP, _FN, _FP =  analysis(p, t, h, w)\n","            TP += _TP[1:n_class]\n","            FN += _FN[1:n_class]\n","            FP += _FP[1:n_class]\n","            \n","    recall = TP / (TP + FN)\n","    precision = TP / (TP + FP)\n","    ious = TP / (TP + FN + FP)\n","    fscore = 2*TP / (2*TP + FN + FP)\n","    total_ious = np.array(total_ious).T  # n_class * val_len\n","    pixel_accs = np.array(pixel_accs).mean()\n","    \n","#     print(\"epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}, recall: {}, precision: {}, fscore: {}\"\\\n","#           .format(epoch, pixel_accs, np.nanmean(ious), ious, recall, precision, fscore))\n","    \n","    f1 = open(score_dir + \"/cls_acc_log.txt\",\"a+\")\n","    f1.write('epoch:'+ str(epoch) + ', pix_acc: ' + str(pixel_accs) + '\\n' )\n","    f2 = open(score_dir + \"/cls_iou_log.txt\",\"a+\")\n","    f2.write('epoch:'+ str(epoch) + ', class ious: ' + str(ious) + '\\n' )\n","    f3 = open(score_dir + \"/mean_iou_log.txt\",\"a+\")\n","    f3.write('epoch:'+ str(epoch) + ', mean IoU: ' + str(np.nanmean(ious)) + '\\n' ) \n","    f4 = open(score_dir + \"/recall_log.txt\",\"a+\")\n","    f4.write('epoch:'+ str(epoch) + ', class recall: ' + str(recall) + '\\n' )\n","    f5 = open(score_dir + \"/precision_log.txt\",\"a+\")\n","    f5.write('epoch:'+ str(epoch) + ', class precision: ' + str(precision) + '\\n' )    \n","    f6 = open(score_dir + \"/fscore_log.txt\",\"a+\")\n","    f6.write('epoch:'+ str(epoch) + ', class fscore: ' + str(fscore) + '\\n' )  \n","    \n","\n","def analysis(pred, target, h, w):\n","    # TP, FN, FP, TN\n","    TP = np.zeros(n_class)\n","    FN = np.zeros(n_class)\n","    FP = np.zeros(n_class)\n","\n","    target = target.reshape(h * w)\n","    pred = pred.reshape(h * w)\n","\n","    con_matrix = confusion_matrix(target, pred,labels = np.arange(0,n_class,1))\n","    con_matrix[0][0] = 0\n","    for i in range(0, n_class):\n","        for j in range(0, n_class):\n","            if i == j:\n","                TP[i] += con_matrix[i][j]\n","            if i != j:\n","                FP[j] += con_matrix[i][j]\n","                FN[i] += con_matrix[i][j]\n","    return TP, FN, FP\n","                \n","def pixel_acc(pred, target):\n","    correct = (pred == target).sum()\n","    total   = (target == target).sum()\n","    return correct / total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7MxcXs2caY3","colab":{"base_uri":"https://localhost:8080/","height":644},"executionInfo":{"status":"error","timestamp":1609218113186,"user_tz":-480,"elapsed":28242,"user":{"displayName":"陳靖霖","photoUrl":"","userId":"01571882859944625156"}},"outputId":"0414d9d8-a823-455c-f4e4-38b543c5f0fe"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["epoch1, iter0, loss: 0.705998957157135\n","epoch1, iter1, loss: 0.6945726871490479\n","epoch1, iter2, loss: 0.6786231398582458\n","epoch1, iter3, loss: 0.6761897206306458\n","epoch1, iter4, loss: 0.6707481741905212\n","epoch1, iter5, loss: 0.6647651195526123\n","epoch1, iter6, loss: 0.6649534106254578\n","epoch1, iter7, loss: 0.663619875907898\n","epoch1, iter8, loss: 0.6579977869987488\n","epoch1, iter9, loss: 0.6676369309425354\n","epoch1, iter10, loss: 0.6683955192565918\n","epoch1, iter11, loss: 0.6660330295562744\n","epoch1, iter12, loss: 0.6625357866287231\n","epoch1, iter13, loss: 0.6609653830528259\n","epoch1, iter14, loss: 0.6623077392578125\n","epoch1, iter15, loss: 0.6570571064949036\n","epoch1, iter16, loss: 0.6546377539634705\n","epoch1, iter17, loss: 0.6635231971740723\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-affa9b01f0e4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch{}, iter{}, loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish epoch {}, time elapsed {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"4svHhapXcQty"},"source":["# Prediction Result"]},{"cell_type":"code","metadata":{"id":"VWVSVUoUxZZh"},"source":["def prediction(model_name):\n","    \n","    # load pretrain models\n","              \n","    vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n","    fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n","    fcn_model = nn.DataParallel(fcn_model)      \n","    \n","    state_dict = torch.load(os.path.join(model_dir, model_name), map_location='cpu')\n","    fcn_model.load_state_dict(state_dict)\n","    \n","    batch = dataiter.next()\n","    # print(batch.shape)\n","    if use_gpu:\n","        inputs = Variable(batch['X'].cuda())\n","    else:\n","        inputs = Variable(batch['X'])\n","    img    = batch['origin'] \n","    label  = batch['l']\n","    \n","    inputs = Variable(batch['X'])\n","    # print(batch['origin'].shape)\n","    output = fcn_model(inputs)\n","    output = output.data.cpu().numpy()\n","\n","    N, _, h, w = output.shape\n","    pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis = 1).reshape(N, h, w)\n","\n","    # show images\n","    plt.figure(figsize = (10, 12))\n","    img = img.numpy()\n","    for i in range(N):\n","        img[i] = cv2.cvtColor(img[i], cv2.COLOR_BGR2RGB)\n","        plt.subplot(N, 3, i*3 + 1)\n","        plt.title(\"origin_img\")\n","        plt.imshow(img[i])\n","        #print(np.unique(_img[i]))\n","\n","        plt.subplot(N, 3, i*3 + 2)\n","        plt.title(\"label_img\")\n","        plt.imshow(label[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n","\n","        plt.subplot(N, 3, i*3 + 3)\n","        plt.title(\"prediction\")\n","        plt.imshow(pred[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n","\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr-BpJ6Yxc-9"},"source":["models_url = \"https://drive.google.com/u/1/uc?id=189Oxxf8SKPlQVcElxFoxyop1HouFv_r6&export=download\"\n","models_name = \"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\"\n","if not os.path.isfile(models_name):\n","    gdown.download(models_url, output=\"models/subt_model/\" + models_name, quiet=False)\n","\n","print(\"Finished downloading models.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-u1T3Ry4cTut"},"source":["# Download_model"]},{"cell_type":"code","metadata":{"id":"uDzXbnPtxkEu"},"source":["for i in range(1000):\r\n","  prediction(\"/content/models/subt_model/FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SeMTgP74cVf0"},"source":["# Try to use the model you just trained to predict!"]},{"cell_type":"code","metadata":{"id":"lGJgIWIJZ6MM"},"source":["prediction(\"/content/models/subt_model/FCNs_subt_model_batch8_epoch0_RMSprop_lr0.0001.pkl\")"],"execution_count":null,"outputs":[]}]}